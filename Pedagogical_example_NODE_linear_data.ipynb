{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Linear Pedagogical Example (Real Data)\n\nUses `bab_datasets` to load multisine data and fit the linear physical model.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# !pip install torchdiffeq git+https://github.com/helonayala/bab_datasets.git\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchdiffeq import odeint\nimport bab_datasets as nod\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 1. LOAD DATA (multisine_05)\n# ==========================================\nvelMethod = \"central\"\n\ndata = nod.load_experiment(\n    \"multisine_05\",\n    preprocess=True,\n    plot=True,\n    end_idx=None,\n    resample_factor=50,\n    zoom_last_n=200,\n    y_dot_method=velMethod,\n)\n\nu, y, y_ref, y_dot = data\n\n# Time vector from sampling_time\nTs = data.sampling_time\nt = np.arange(len(u)) * Ts\n\n# Stack states: [position, velocity]\ny_sim = np.column_stack([y, y_dot])\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 2. PREPARE TENSORS\n# ==========================================\nt_tensor = torch.tensor(t, dtype=torch.float32).to(device)\nu_tensor = torch.tensor(u, dtype=torch.float32).reshape(-1, 1).to(device)\ny_tensor = torch.tensor(y_sim, dtype=torch.float32).to(device)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 3. MODEL DEFINITION\n# ==========================================\nclass LinearPhysODE(nn.Module):\n    # J * thdd + R * thd + K * (th + delta) = Tau * V\n    # States: [th, thd]\n    def __init__(self):\n        super().__init__()\n        self.log_J = nn.Parameter(torch.tensor(np.log(0.1), dtype=torch.float32))\n        self.log_R = nn.Parameter(torch.tensor(np.log(0.1), dtype=torch.float32))\n        self.log_K = nn.Parameter(torch.tensor(np.log(1.0), dtype=torch.float32))\n        self.delta = nn.Parameter(torch.tensor(0.0, dtype=torch.float32))\n        self.log_Tau = nn.Parameter(torch.tensor(np.log(1.0), dtype=torch.float32))\n\n        self.u_series = None\n        self.t_series = None\n        self.batch_start_times = None\n\n    def get_params(self):\n        J = torch.exp(self.log_J)\n        R = torch.exp(self.log_R)\n        K = torch.exp(self.log_K)\n        Tau = torch.exp(self.log_Tau)\n        return J, R, K, self.delta, Tau\n\n    def forward(self, t, x):\n        J, R, K, delta, Tau = self.get_params()\n\n        if self.batch_start_times is not None:\n            t_abs = self.batch_start_times + t\n        else:\n            t_abs = t * torch.ones_like(x[:, 0:1])\n\n        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n\n        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n\n        denom = (t2 - t1)\n        denom[denom < 1e-6] = 1.0\n        alpha = (t_abs - t1) / denom\n        u_t = u1 + alpha * (u2 - u1)\n\n        th, thd = x[:, 0:1], x[:, 1:2]\n        thdd = (Tau * u_t - R * thd - K * (th + delta)) / J\n        return torch.cat([thd, thdd], dim=1)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 4. TRAINING FUNCTION\n# ==========================================\n\ndef train_model(model, name, epochs=500, lr=0.02):\n    print(f\"--- Training {name} ---\")\n    model.to(device)\n    model.u_series = u_tensor\n    model.t_series = t_tensor\n\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    K_STEPS = 20\n    BATCH_SIZE = 128\n    dt_local = (t_tensor[1] - t_tensor[0]).item()\n    t_eval = torch.arange(0, K_STEPS * dt_local, dt_local, device=device)\n\n    for epoch in range(epochs + 1):\n        optimizer.zero_grad()\n\n        start_idx = np.random.randint(0, len(t_tensor) - K_STEPS, size=BATCH_SIZE)\n        x0 = y_tensor[start_idx]\n        model.batch_start_times = t_tensor[start_idx].reshape(-1, 1)\n\n        pred_state = odeint(model, x0, t_eval, method='rk4')\n\n        batch_targets = []\n        for i in start_idx:\n            batch_targets.append(y_tensor[i:i + K_STEPS])\n        y_target = torch.stack(batch_targets, dim=1)\n\n        loss = torch.mean((pred_state - y_target) ** 2)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch} | Loss: {loss.item():.6f}\")\n\n    return model\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 5. TRAIN PHYSICAL MODEL\n# ==========================================\nphys_model = LinearPhysODE()\nphys_model = train_model(phys_model, \"Linear Physical Model\", epochs=500, lr=0.02)\n\nJ, R, K, delta, Tau = phys_model.get_params()\nprint(f\"identified: J={J.item():.4f}, R={R.item():.4f}, K={K.item():.4f}, delta={delta.item():.4f}, Tau={Tau.item():.4f}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 6. FULL SIMULATION & COMPARISON\n# ==========================================\nprint(\"--- Running Full Data Simulation ---\")\n\nwith torch.no_grad():\n    phys_model.batch_start_times = torch.zeros(1, 1).to(device)\n    x0 = y_tensor[0].unsqueeze(0)\n\n    full_pred = odeint(phys_model, x0, t_tensor, method='rk4')\n    full_pred = full_pred.squeeze(1).cpu().numpy()\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(t, y_sim[:, 0], 'k-', alpha=0.4, linewidth=3, label='Measured')\nplt.plot(t, full_pred[:, 0], 'r--', linewidth=1.5, label='Simulated')\nplt.title(\"Position Comparison\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Position\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(t, y_sim[:, 1], 'k-', alpha=0.4, linewidth=3, label='Measured')\nplt.plot(t, full_pred[:, 1], 'r--', linewidth=1.5, label='Simulated')\nplt.title(\"Velocity Comparison\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Velocity\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==========================================\n# 7. RESIDUALS (Measured - Simulated)\n# ==========================================\nres_pos = y_sim[:, 0] - full_pred[:, 0]\nres_vel = y_sim[:, 1] - full_pred[:, 1]\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(2, 1, 1)\nplt.plot(t, res_pos, 'k-', linewidth=1.5, label='Position residual')\nplt.axhline(0, color='gray', linewidth=1)\nplt.title(\"Residuals\")\nplt.ylabel(\"Position residual\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(t, res_vel, 'k-', linewidth=1.5, label='Velocity residual')\nplt.axhline(0, color='gray', linewidth=1)\nplt.ylabel(\"Velocity residual\")\nplt.xlabel(\"Time (s)\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
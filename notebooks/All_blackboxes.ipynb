{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# BAB Dataset: Enhanced Neural ODE & CT-ESN Models\n",
    "\n",
    "**Multi-dataset training** on multisine + swept sine signals, with **holdout testing** on random steps.  \n",
    "Includes ablation study over prediction horizon ($K$) and batch size ($B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 0) Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "import bab_datasets as nod\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load",
   "metadata": {},
   "source": [
    "## 1) Load Training & Test Data\n",
    "\n",
    "**Training data** — multi-dataset: `multisine_05`, `multisine_06`, `swept_sine`  \n",
    "**Test data** (holdout) — unseen excitation type: `random_steps_01`, `random_steps_02`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "velMethod = \"central\"\n",
    "RESAMPLE = 50\n",
    "\n",
    "# ---------- Training datasets ----------\n",
    "train_names = [\"multisine_05\", \"multisine_06\", \"swept_sine\"]\n",
    "train_datasets = []\n",
    "\n",
    "for ds_name in train_names:\n",
    "    d = nod.load_experiment(\n",
    "        ds_name, preprocess=True, plot=False,\n",
    "        resample_factor=RESAMPLE, y_dot_method=velMethod,\n",
    "    )\n",
    "    u_d, y_d, _, ydot_d = d\n",
    "    Ts_d = d.sampling_time\n",
    "    t_d = np.arange(len(u_d)) * Ts_d\n",
    "    train_datasets.append({\n",
    "        \"name\": ds_name, \"u\": u_d, \"y\": y_d, \"y_dot\": ydot_d,\n",
    "        \"t\": t_d, \"Ts\": Ts_d,\n",
    "        \"y_sim\": np.column_stack([y_d, ydot_d]),\n",
    "    })\n",
    "    print(f\"  Train: {ds_name:>15s}  |  N={len(u_d):5d}  |  Ts={Ts_d:.5f}s  |  T={t_d[-1]:.1f}s\")\n",
    "\n",
    "# ---------- Holdout test datasets ----------\n",
    "test_names = [\"random_steps_01\", \"random_steps_02\", \"random_steps_03\", \"random_steps_04\"]\n",
    "test_datasets = []\n",
    "\n",
    "for ds_name in test_names:\n",
    "    d = nod.load_experiment(\n",
    "        ds_name, preprocess=True, plot=False,\n",
    "        resample_factor=RESAMPLE, y_dot_method=velMethod,\n",
    "    )\n",
    "    u_d, y_d, _, ydot_d = d\n",
    "    Ts_d = d.sampling_time\n",
    "    t_d = np.arange(len(u_d)) * Ts_d\n",
    "    test_datasets.append({\n",
    "        \"name\": ds_name, \"u\": u_d, \"y\": y_d, \"y_dot\": ydot_d,\n",
    "        \"t\": t_d, \"Ts\": Ts_d,\n",
    "        \"y_sim\": np.column_stack([y_d, ydot_d]),\n",
    "    })\n",
    "    print(f\"  Test:  {ds_name:>15s}  |  N={len(u_d):5d}  |  Ts={Ts_d:.5f}s  |  T={t_d[-1]:.1f}s\")\n",
    "\n",
    "# Keep the first training set as primary reference (for backward compat)\n",
    "u, y, y_dot = train_datasets[0][\"u\"], train_datasets[0][\"y\"], train_datasets[0][\"y_dot\"]\n",
    "t, Ts = train_datasets[0][\"t\"], train_datasets[0][\"Ts\"]\n",
    "y_sim = train_datasets[0][\"y_sim\"]\n",
    "\n",
    "# Quick overview\n",
    "fig, axes = plt.subplots(len(train_datasets), 3, figsize=(16, 3*len(train_datasets)),\n",
    "                          sharex=False, squeeze=False)\n",
    "for i, ds in enumerate(train_datasets):\n",
    "    axes[i, 0].plot(ds[\"t\"], ds[\"u\"]); axes[i, 0].set_ylabel(\"u\")\n",
    "    axes[i, 0].set_title(ds[\"name\"])\n",
    "    axes[i, 1].plot(ds[\"t\"], ds[\"y\"]); axes[i, 1].set_ylabel(\"y\")\n",
    "    axes[i, 2].plot(ds[\"t\"], ds[\"y_dot\"]); axes[i, 2].set_ylabel(\"ẏ\")\n",
    "    for ax in axes[i]:\n",
    "        ax.grid(True, alpha=0.3)\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "fig.suptitle(\"Training Datasets\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor_prep",
   "metadata": {},
   "source": [
    "## 2) Tensor Prep (multi-dataset)\n",
    "\n",
    "Each training dataset is stored as its own set of tensors so the training loop can randomly sample across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-dataset tensors for multi-dataset training ---\n",
    "train_tensors = []\n",
    "for ds in train_datasets:\n",
    "    tt = torch.tensor(ds[\"t\"], dtype=torch.float32).to(device)\n",
    "    ut = torch.tensor(ds[\"u\"], dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    yt = torch.tensor(ds[\"y_sim\"], dtype=torch.float32).to(device)\n",
    "    train_tensors.append({\"t\": tt, \"u\": ut, \"y\": yt, \"name\": ds[\"name\"]})\n",
    "\n",
    "# --- Test tensors ---\n",
    "test_tensors = []\n",
    "for ds in test_datasets:\n",
    "    tt = torch.tensor(ds[\"t\"], dtype=torch.float32).to(device)\n",
    "    ut = torch.tensor(ds[\"u\"], dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    yt = torch.tensor(ds[\"y_sim\"], dtype=torch.float32).to(device)\n",
    "    test_tensors.append({\"t\": tt, \"u\": ut, \"y\": yt, \"name\": ds[\"name\"]})\n",
    "\n",
    "# Primary tensors (backward compat for model.u_series / .t_series during simulation)\n",
    "t_tensor = train_tensors[0][\"t\"]\n",
    "u_tensor = train_tensors[0][\"u\"]\n",
    "y_tensor = train_tensors[0][\"y\"]\n",
    "\n",
    "print(f\"Training on {len(train_tensors)} datasets, testing on {len(test_tensors)} holdout datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models",
   "metadata": {},
   "source": [
    "## 3) Model Definitions\n",
    "\n",
    "### 3a) Vanilla NODE Baseline & Ablation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNODE(nn.Module):\n",
    "    \"\"\"Vanilla Neural ODE: learns both derivatives (no kinematic prior).\"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "        self.u_series = None\n",
    "        self.t_series = None\n",
    "        self.batch_start_times = None\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.batch_start_times is not None:\n",
    "            t_abs = self.batch_start_times + t\n",
    "        else:\n",
    "            t_abs = t * torch.ones_like(x[:, 0:1])\n",
    "\n",
    "        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n",
    "        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n",
    "        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n",
    "        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n",
    "        denom = (t2 - t1)\n",
    "        denom[denom < 1e-6] = 1.0\n",
    "        alpha = (t_abs - t1) / denom\n",
    "        u_t = u1 + alpha * (u2 - u1)\n",
    "\n",
    "        nn_input = torch.cat([x, u_t], dim=1)\n",
    "        return self.net(nn_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced_models_section",
   "metadata": {},
   "source": [
    "### 3b) Enhanced Models - CT-ESN\n",
    "\n",
    "**CT-ESN-Kinematic**: Only predicts acceleration, hardcodes x_0' = x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced_ctesn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTimeESN_Kinematic(nn.Module):\n",
    "    \"\"\"\n",
    "    ENHANCED CT-ESN: Kinematic structure (only predicts acceleration).\n",
    "    FIX: lower leak_rate, learnable input/reservoir scaling, train all params.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim=2, input_dim=1, reservoir_dim=200,\n",
    "                 spectral_radius=0.9, input_scale=0.5, leak_rate=1.0):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.reservoir_dim = reservoir_dim\n",
    "\n",
    "        # Learnable leak rate (initialised moderate — prevents stiff ODE)\n",
    "        self.leak_rate = nn.Parameter(torch.tensor(leak_rate))\n",
    "\n",
    "        # Fixed reservoir matrix\n",
    "        W = torch.randn(reservoir_dim, reservoir_dim) * 0.1\n",
    "        mask = (torch.rand_like(W) < 0.8)\n",
    "        W[mask] = 0.0\n",
    "        eigvals = torch.linalg.eigvals(W).abs()\n",
    "        if eigvals.max() > 0:\n",
    "            W = W * (spectral_radius / eigvals.max())\n",
    "        self.register_buffer('W_res', W)\n",
    "\n",
    "        # Learnable input weights (not frozen)\n",
    "        self.W_in = nn.Parameter(\n",
    "            torch.randn(reservoir_dim, state_dim + input_dim) * input_scale\n",
    "        )\n",
    "\n",
    "        # Output: acceleration (1D)\n",
    "        self.W_out = nn.Linear(reservoir_dim + state_dim + input_dim, 1, bias=True)\n",
    "\n",
    "        self.u_series = None\n",
    "        self.t_series = None\n",
    "        self.batch_start_times = None\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        x = z[:, :self.state_dim]\n",
    "        r = z[:, self.state_dim:]\n",
    "\n",
    "        position, velocity = x[:, 0:1], x[:, 1:2]\n",
    "\n",
    "        if self.batch_start_times is not None:\n",
    "            t_abs = self.batch_start_times + t\n",
    "        else:\n",
    "            t_abs = t * torch.ones_like(position)\n",
    "\n",
    "        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n",
    "        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n",
    "        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n",
    "        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n",
    "        denom = (t2 - t1)\n",
    "        denom[denom < 1e-6] = 1.0\n",
    "        alpha = (t_abs - t1) / denom\n",
    "        u_t = u1 + alpha * (u2 - u1)\n",
    "\n",
    "        xu = torch.cat([x, u_t], dim=1)\n",
    "        # Clamp leak_rate to prevent it going negative or too large\n",
    "        lr = torch.clamp(self.leak_rate, 0.1, 10.0)\n",
    "        dr = lr * (-r + torch.tanh(r @ self.W_res.T + xu @ self.W_in.T))\n",
    "\n",
    "        # Readout: use reservoir state + direct state/input skip connection\n",
    "        readout_input = torch.cat([r, x, u_t], dim=1)\n",
    "        acceleration = self.W_out(readout_input)\n",
    "\n",
    "        # KINEMATIC STRUCTURE: hardcode x_0' = x_1\n",
    "        dx = torch.cat([velocity, acceleration], dim=1)\n",
    "\n",
    "        return torch.cat([dx, dr], dim=1)\n",
    "\n",
    "    def init_reservoir(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.reservoir_dim, device=self.W_res.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced_cnode_section",
   "metadata": {},
   "source": [
    "### 3c) Enhanced Models - NODE\n",
    "\n",
    "**Structured NODE**: Kinematic constraint with single NN.  \n",
    "**Adaptive NODE**: Base dynamics + near-zero residual correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced_cnode",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredNODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Structured NODE: Kinematic constraint, single NN predicts acceleration.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Single network (same as Structured-BB)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.u_series = None\n",
    "        self.t_series = None\n",
    "        self.batch_start_times = None\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.batch_start_times is not None:\n",
    "            t_abs = self.batch_start_times + t\n",
    "        else:\n",
    "            t_abs = t * torch.ones_like(x[:, 0:1])\n",
    "\n",
    "        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n",
    "        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n",
    "        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n",
    "        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n",
    "        denom = (t2 - t1)\n",
    "        denom[denom < 1e-6] = 1.0\n",
    "        alpha = (t_abs - t1) / denom\n",
    "        u_t = u1 + alpha * (u2 - u1)\n",
    "\n",
    "        th, thd = x[:, 0:1], x[:, 1:2]\n",
    "        nn_input = torch.cat([th, thd, u_t], dim=1)\n",
    "        thdd = self.net(nn_input)\n",
    "\n",
    "        return torch.cat([thd, thdd], dim=1)\n",
    "\n",
    "\n",
    "class AdaptiveNODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive NODE: Base dynamics NN + small residual correction NN.\n",
    "    Both paths receive [θ, θ̇, u]. Residual initialised near zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main dynamics network: takes [th, thd, u] — same as Structured-BB\n",
    "        self.dynamics_net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.05),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        # Adaptive residual: state-dependent correction to acceleration\n",
    "        self.adaptive_residual = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "        # Initialise residual path near zero so it doesn't destabilise early training\n",
    "        with torch.no_grad():\n",
    "            self.adaptive_residual[-1].weight.mul_(0.01)\n",
    "            self.adaptive_residual[-1].bias.zero_()\n",
    "\n",
    "        self.u_series = None\n",
    "        self.t_series = None\n",
    "        self.batch_start_times = None\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.batch_start_times is not None:\n",
    "            t_abs = self.batch_start_times + t\n",
    "        else:\n",
    "            t_abs = t * torch.ones_like(x[:, 0:1])\n",
    "\n",
    "        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n",
    "        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n",
    "        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n",
    "        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n",
    "        denom = (t2 - t1)\n",
    "        denom[denom < 1e-6] = 1.0\n",
    "        alpha = (t_abs - t1) / denom\n",
    "        u_t = u1 + alpha * (u2 - u1)\n",
    "\n",
    "        th, thd = x[:, 0:1], x[:, 1:2]\n",
    "        nn_input = torch.cat([th, thd, u_t], dim=1)\n",
    "\n",
    "        # Base acceleration (same structure as Structured-BB)\n",
    "        base_accel = self.dynamics_net(nn_input)\n",
    "        # Small state+input-dependent residual correction\n",
    "        residual = self.adaptive_residual(nn_input)\n",
    "\n",
    "        thdd = base_accel + residual\n",
    "        return torch.cat([thd, thdd], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469986e7",
   "metadata": {},
   "source": [
    "### 3e) Bayesian Neural ODE (BNN-NODE)\n",
    "\n",
    "**Bayesian NODE** with **structured kinematic constraint** ($\\dot x_0 = x_1$).  \n",
    "Each weight/bias is parameterised by a mean $\\mu$ and log-variance $\\log\\sigma^2$ (mean-field Gaussian).  \n",
    "During training, weights are sampled via the reparameterisation trick, and the loss combines the\n",
    "data likelihood with a KL divergence regulariser (ELBO).  \n",
    "At inference time, **multiple forward passes** yield a **predictive distribution** (epistemic uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian linear layer with mean-field Gaussian variational posterior.\n",
    "    Weight ~ N(mu_w, sigma_w^2), Bias ~ N(mu_b, sigma_b^2).\n",
    "    Uses reparameterisation trick: w = mu + sigma * eps, eps ~ N(0,1).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Variational parameters — weight\n",
    "        self.mu_w = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.log_sigma_w = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        # Variational parameters — bias\n",
    "        self.mu_b = nn.Parameter(torch.empty(out_features))\n",
    "        self.log_sigma_b = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # Prior: fixed zero-mean isotropic Gaussian\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.prior_log_sigma = math.log(prior_sigma)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Initialise means like Kaiming uniform\n",
    "        nn.init.kaiming_uniform_(self.mu_w, a=math.sqrt(5))\n",
    "        fan_in = self.in_features\n",
    "        bound = 1.0 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.mu_b, -bound, bound)\n",
    "        # Initialise log_sigma so that sigma starts small (~0.01)\n",
    "        nn.init.constant_(self.log_sigma_w, -4.6)\n",
    "        nn.init.constant_(self.log_sigma_b, -4.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma_w = torch.exp(self.log_sigma_w)\n",
    "        sigma_b = torch.exp(self.log_sigma_b)\n",
    "\n",
    "        if self.training:\n",
    "            eps_w = torch.randn_like(self.mu_w)\n",
    "            eps_b = torch.randn_like(self.mu_b)\n",
    "            weight = self.mu_w + sigma_w * eps_w\n",
    "            bias = self.mu_b + sigma_b * eps_b\n",
    "        else:\n",
    "            weight = self.mu_w\n",
    "            bias = self.mu_b\n",
    "\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"\n",
    "        Closed-form KL(q || p) for two diagonal Gaussians.\n",
    "        q = N(mu, sigma^2), p = N(0, prior_sigma^2).\n",
    "        \"\"\"\n",
    "        sigma_w = torch.exp(self.log_sigma_w)\n",
    "        sigma_b = torch.exp(self.log_sigma_b)\n",
    "\n",
    "        kl_w = (\n",
    "            torch.log(self.prior_sigma / sigma_w)\n",
    "            + (sigma_w**2 + self.mu_w**2) / (2 * self.prior_sigma**2)\n",
    "            - 0.5\n",
    "        ).sum()\n",
    "\n",
    "        kl_b = (\n",
    "            torch.log(self.prior_sigma / sigma_b)\n",
    "            + (sigma_b**2 + self.mu_b**2) / (2 * self.prior_sigma**2)\n",
    "            - 0.5\n",
    "        ).sum()\n",
    "\n",
    "        return kl_w + kl_b\n",
    "\n",
    "\n",
    "class BayesianNODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Neural ODE with kinematic constraint.\n",
    "    - dx_0/dt = x_1  (hardcoded)\n",
    "    - dx_1/dt = BNN([theta, theta_dot, u])\n",
    "    \n",
    "    During training, each forward pass samples new weights → stochastic gradients\n",
    "    implement Bayes by Backprop (Blundell et al., 2015).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.prior_sigma = prior_sigma\n",
    "\n",
    "        self.fc1 = BayesianLinear(3, hidden_dim, prior_sigma)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, hidden_dim, prior_sigma)\n",
    "        self.fc3 = BayesianLinear(hidden_dim, hidden_dim // 2, prior_sigma)\n",
    "        self.fc4 = BayesianLinear(hidden_dim // 2, 1, prior_sigma)\n",
    "\n",
    "        self.u_series = None\n",
    "        self.t_series = None\n",
    "        self.batch_start_times = None\n",
    "\n",
    "    def _net(self, x):\n",
    "        \"\"\"Bayesian acceleration network.\"\"\"\n",
    "        x = torch.selu(self.fc1(x))\n",
    "        x = torch.selu(self.fc2(x))\n",
    "        x = torch.selu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.batch_start_times is not None:\n",
    "            t_abs = self.batch_start_times + t\n",
    "        else:\n",
    "            t_abs = t * torch.ones_like(x[:, 0:1])\n",
    "\n",
    "        k_idx = torch.searchsorted(self.t_series, t_abs.reshape(-1), right=True)\n",
    "        k_idx = torch.clamp(k_idx, 1, len(self.t_series) - 1)\n",
    "        t1, t2 = self.t_series[k_idx - 1].unsqueeze(1), self.t_series[k_idx].unsqueeze(1)\n",
    "        u1, u2 = self.u_series[k_idx - 1], self.u_series[k_idx]\n",
    "        denom = (t2 - t1)\n",
    "        denom[denom < 1e-6] = 1.0\n",
    "        alpha = (t_abs - t1) / denom\n",
    "        u_t = u1 + alpha * (u2 - u1)\n",
    "\n",
    "        th, thd = x[:, 0:1], x[:, 1:2]\n",
    "        nn_input = torch.cat([th, thd, u_t], dim=1)\n",
    "        thdd = self._net(nn_input)\n",
    "\n",
    "        return torch.cat([thd, thdd], dim=1)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Sum KL divergence over all Bayesian layers.\"\"\"\n",
    "        return sum(\n",
    "            m.kl_divergence()\n",
    "            for m in self.modules()\n",
    "            if isinstance(m, BayesianLinear)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"BayesianLinear + BayesianNODE defined ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a08cd",
   "metadata": {},
   "source": [
    "### 3d) Controlled NODE\n",
    "Disappointing results so far with this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural CDE (commented out — not used) ---\n",
    "# import torchcde\n",
    "#\n",
    "#\n",
    "# class CDEFunc(nn.Module):\n",
    "#     \"\"\"GRU-style gated vector field for Neural CDE.\"\"\"\n",
    "#     def __init__(self, hidden_dim, input_channels):\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.input_channels = input_channels\n",
    "#         self.gate_net = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 64), nn.Tanh(),\n",
    "#             nn.Linear(64, hidden_dim * input_channels), nn.Sigmoid(),\n",
    "#         )\n",
    "#         self.update_net = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, 64), nn.Tanh(),\n",
    "#             nn.Linear(64, hidden_dim * input_channels), nn.Tanh(),\n",
    "#         )\n",
    "#     def forward(self, t, z):\n",
    "#         gate = self.gate_net(z).view(z.size(0), self.hidden_dim, self.input_channels)\n",
    "#         update = self.update_net(z).view(z.size(0), self.hidden_dim, self.input_channels)\n",
    "#         return gate * update\n",
    "#\n",
    "#\n",
    "# class NeuralCDE(nn.Module):\n",
    "#     \"\"\"Neural CDE for system identification — direct state prediction.\"\"\"\n",
    "#     def __init__(self, hidden_dim=64, input_channels=2):\n",
    "#         super().__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.input_channels = input_channels\n",
    "#         self.initial_net = nn.Sequential(nn.Linear(2, hidden_dim), nn.Tanh())\n",
    "#         self.cde_func = CDEFunc(hidden_dim, input_channels)\n",
    "#         self.norm = nn.LayerNorm(hidden_dim)\n",
    "#         self.readout = nn.Linear(hidden_dim, 2)\n",
    "#     def forward(self, X_interp, t_span, x0):\n",
    "#         z0 = self.initial_net(x0)\n",
    "#         z_trajectory = torchcde.cdeint(\n",
    "#             X=X_interp, z0=z0, func=self.cde_func, t=t_span,\n",
    "#             method='rk4',\n",
    "#             options={'step_size': (t_span[1] - t_span[0]).item()},\n",
    "#         )\n",
    "#         z_trajectory = z_trajectory.permute(1, 0, 2)\n",
    "#         z_normed = self.norm(z_trajectory)\n",
    "#         delta = self.readout(z_normed)\n",
    "#         pred = x0.unsqueeze(0) + delta\n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_functions",
   "metadata": {},
   "source": [
    "## 4) Training Functions (multi-dataset)\n",
    "\n",
    "Each epoch randomly selects a dataset, then samples a batch of initial conditions from it.\n",
    "This encourages the model to learn dynamics that generalise across different excitation signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_multi(model, name, epochs=500, lr=0.02, obs_dim=2,\n",
    "                      k_steps=20, batch_size=128, log_every=100):\n",
    "    \"\"\"\n",
    "    Multi-dataset training for Neural ODE models.\n",
    "    Each epoch randomly picks a dataset, then samples a batch of windows.\n",
    "    \"\"\"\n",
    "    print(f\"--- Training {name}  (K={k_steps}, B={batch_size}) ---\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "        # Pick a random training dataset\n",
    "        ds = train_tensors[np.random.randint(len(train_tensors))]\n",
    "        t_ds, u_ds, y_ds = ds[\"t\"], ds[\"u\"], ds[\"y\"]\n",
    "        dt_local = (t_ds[1] - t_ds[0]).item()\n",
    "        t_eval = torch.arange(0, k_steps * dt_local, dt_local, device=device)\n",
    "\n",
    "        model.u_series = u_ds\n",
    "        model.t_series = t_ds\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        start_idx = np.random.randint(0, len(t_ds) - k_steps, size=batch_size)\n",
    "        x0 = y_ds[start_idx]\n",
    "        model.batch_start_times = t_ds[start_idx].reshape(-1, 1)\n",
    "\n",
    "        pred_state = odeint(model, x0, t_eval, method='rk4')\n",
    "        pred_obs = pred_state[..., :obs_dim]\n",
    "\n",
    "        batch_targets = torch.stack([y_ds[i:i + k_steps] for i in start_idx], dim=1)\n",
    "        loss = torch.mean((pred_obs - batch_targets) ** 2)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % log_every == 0:\n",
    "            print(f\"  Epoch {epoch:>5d} | Loss: {loss.item():.6f}  ({ds['name']})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_ctesn_multi(model, name, epochs=2000, lr=0.01,\n",
    "                      k_steps=20, batch_size=128, log_every=100):\n",
    "    \"\"\"Multi-dataset training for kinematic CT-ESN.\n",
    "    FIX: train ALL learnable parameters (W_in, leak_rate, W_out), not just W_out.\n",
    "    \"\"\"\n",
    "    print(f\"--- Training {name}  (K={k_steps}, B={batch_size}) ---\")\n",
    "    model.to(device)\n",
    "    # FIX: train all parameters, not just W_out\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    state_dim = model.state_dim\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "        ds = train_tensors[np.random.randint(len(train_tensors))]\n",
    "        t_ds, u_ds, y_ds = ds[\"t\"], ds[\"u\"], ds[\"y\"]\n",
    "        dt_local = (t_ds[1] - t_ds[0]).item()\n",
    "        t_eval = torch.arange(0, k_steps * dt_local, dt_local, device=device)\n",
    "\n",
    "        model.u_series = u_ds\n",
    "        model.t_series = t_ds\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        start_idx = np.random.randint(0, len(t_ds) - k_steps, size=batch_size)\n",
    "\n",
    "        x0 = y_ds[start_idx, :state_dim]\n",
    "        r0 = model.init_reservoir(batch_size)\n",
    "        z0 = torch.cat([x0, r0], dim=1)\n",
    "        model.batch_start_times = t_ds[start_idx].reshape(-1, 1)\n",
    "\n",
    "        pred_z = odeint(model, z0, t_eval, method='rk4')\n",
    "        pred_obs = pred_z[..., :state_dim]\n",
    "\n",
    "        batch_targets = torch.stack(\n",
    "            [y_ds[i:i + k_steps, :state_dim] for i in start_idx], dim=1)\n",
    "        loss = torch.mean((pred_obs - batch_targets) ** 2)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % log_every == 0:\n",
    "            print(f\"  Epoch {epoch:>5d} | Loss: {loss.item():.6f}  ({ds['name']})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def simulate_model(model, ds_tensors, is_esn=False, esn_model=None):\n",
    "    \"\"\"\n",
    "    Full-trajectory simulation on a given dataset.\n",
    "    Returns (t_np, y_sim_np, pred_np).\n",
    "    \"\"\"\n",
    "    t_ds, u_ds, y_ds = ds_tensors[\"t\"], ds_tensors[\"u\"], ds_tensors[\"y\"]\n",
    "    model.eval()\n",
    "    model.u_series = u_ds\n",
    "    model.t_series = t_ds\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.batch_start_times = torch.zeros(1, 1).to(device)\n",
    "        if is_esn:\n",
    "            x0 = y_ds[0, :esn_model.state_dim].unsqueeze(0)\n",
    "            r0 = esn_model.init_reservoir(1)\n",
    "            z0 = torch.cat([x0, r0], dim=1)\n",
    "            pred = odeint(model, z0, t_ds, method='rk4').squeeze(1).cpu().numpy()\n",
    "            pred = pred[:, :esn_model.state_dim]\n",
    "        else:\n",
    "            x0 = y_ds[0].unsqueeze(0)\n",
    "            pred = odeint(model, x0, t_ds, method='rk4').squeeze(1).cpu().numpy()\n",
    "\n",
    "    return t_ds.cpu().numpy(), y_ds.cpu().numpy(), pred\n",
    "\n",
    "\n",
    "# --- Neural CDE training & simulation (commented out — not used) ---\n",
    "# def train_cde_multi(model, name, epochs=500, lr=0.005,\n",
    "#                     k_steps=20, batch_size=128, log_every=100):\n",
    "#     ...\n",
    "#\n",
    "# def simulate_cde(model, ds_tensors):\n",
    "#     ...\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute RMSE, R², FIT% for position and velocity.\"\"\"\n",
    "    res_pos = y_true[:, 0] - y_pred[:, 0]\n",
    "    res_vel = y_true[:, 1] - y_pred[:, 1]\n",
    "    rmse_pos = np.sqrt(np.mean(res_pos**2))\n",
    "    rmse_vel = np.sqrt(np.mean(res_vel**2))\n",
    "    ss_res_pos = np.sum(res_pos**2)\n",
    "    ss_tot_pos = np.sum((y_true[:, 0] - np.mean(y_true[:, 0]))**2)\n",
    "    ss_res_vel = np.sum(res_vel**2)\n",
    "    ss_tot_vel = np.sum((y_true[:, 1] - np.mean(y_true[:, 1]))**2)\n",
    "    r2_pos = 1 - ss_res_pos / ss_tot_pos if ss_tot_pos > 0 else np.nan\n",
    "    r2_vel = 1 - ss_res_vel / ss_tot_vel if ss_tot_vel > 0 else np.nan\n",
    "    fit_pos = 100 * (1 - np.linalg.norm(res_pos) / np.linalg.norm(y_true[:, 0] - np.mean(y_true[:, 0])))\n",
    "    fit_vel = 100 * (1 - np.linalg.norm(res_vel) / np.linalg.norm(y_true[:, 1] - np.mean(y_true[:, 1])))\n",
    "    return {\n",
    "        \"rmse_pos\": rmse_pos, \"rmse_vel\": rmse_vel,\n",
    "        \"r2_pos\": r2_pos, \"r2_vel\": r2_vel,\n",
    "        \"fit_pos\": fit_pos, \"fit_vel\": fit_vel,\n",
    "        \"res_pos\": res_pos, \"res_vel\": res_vel,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c48885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bnn_multi(model, name, epochs=500, lr=0.01, obs_dim=2,\n",
    "                    k_steps=20, batch_size=128, log_every=100,\n",
    "                    kl_weight=1e-4, n_samples=3):\n",
    "    \"\"\"\n",
    "    Multi-dataset training for Bayesian NODE (Bayes by Backprop).\n",
    "    Loss = E_q[ MSE ] + kl_weight * KL(q || p)\n",
    "    We average the MSE over `n_samples` weight draws per step for lower variance.\n",
    "    \"\"\"\n",
    "    print(f\"--- Training {name}  (K={k_steps}, B={batch_size}, \"\n",
    "          f\"KL_w={kl_weight}, n_samples={n_samples}) ---\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "        # Pick a random training dataset\n",
    "        ds = train_tensors[np.random.randint(len(train_tensors))]\n",
    "        t_ds, u_ds, y_ds = ds[\"t\"], ds[\"u\"], ds[\"y\"]\n",
    "        dt_local = (t_ds[1] - t_ds[0]).item()\n",
    "        t_eval = torch.arange(0, k_steps * dt_local, dt_local, device=device)\n",
    "\n",
    "        model.u_series = u_ds\n",
    "        model.t_series = t_ds\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        start_idx = np.random.randint(0, len(t_ds) - k_steps, size=batch_size)\n",
    "        x0 = y_ds[start_idx]\n",
    "        model.batch_start_times = t_ds[start_idx].reshape(-1, 1)\n",
    "\n",
    "        batch_targets = torch.stack([y_ds[i:i + k_steps] for i in start_idx], dim=1)\n",
    "\n",
    "        # Average MSE over n_samples weight draws (reduces gradient variance)\n",
    "        mse_accum = 0.0\n",
    "        model.train()\n",
    "        for _ in range(n_samples):\n",
    "            pred_state = odeint(model, x0, t_eval, method='rk4')\n",
    "            pred_obs = pred_state[..., :obs_dim]\n",
    "            mse_accum = mse_accum + torch.mean((pred_obs - batch_targets) ** 2)\n",
    "        mse_loss = mse_accum / n_samples\n",
    "\n",
    "        # KL divergence — scaled by dataset size\n",
    "        N_total = sum(len(ds_t[\"t\"]) for ds_t in train_tensors)\n",
    "        kl_loss = model.kl_divergence()\n",
    "        loss = mse_loss + kl_weight * kl_loss / N_total\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % log_every == 0:\n",
    "            print(f\"  Epoch {epoch:>5d} | MSE: {mse_loss.item():.6f} | \"\n",
    "                  f\"KL: {kl_loss.item():.1f} | Total: {loss.item():.6f}  ({ds['name']})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def simulate_bnn_with_uncertainty(model, ds_tensors, n_mc=20):\n",
    "    \"\"\"\n",
    "    Simulate a Bayesian NODE multiple times (MC dropout / weight sampling)\n",
    "    to obtain predictive mean and ±2σ uncertainty bands.\n",
    "    Returns (t_np, y_true_np, mean_pred, std_pred).\n",
    "    \"\"\"\n",
    "    t_ds, u_ds, y_ds = ds_tensors[\"t\"], ds_tensors[\"u\"], ds_tensors[\"y\"]\n",
    "    model.u_series = u_ds\n",
    "    model.t_series = t_ds\n",
    "\n",
    "    preds = []\n",
    "    model.train()  # keep stochastic sampling active\n",
    "    with torch.no_grad():\n",
    "        model.batch_start_times = torch.zeros(1, 1).to(device)\n",
    "        x0 = y_ds[0].unsqueeze(0)\n",
    "        for _ in range(n_mc):\n",
    "            pred = odeint(model, x0, t_ds, method='rk4').squeeze(1).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.stack(preds, axis=0)  # (n_mc, T, 2)\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    std_pred = preds.std(axis=0)\n",
    "\n",
    "    model.eval()  # restore deterministic mode\n",
    "    return t_ds.cpu().numpy(), y_ds.cpu().numpy(), mean_pred, std_pred\n",
    "\n",
    "\n",
    "print(\"BNN training & uncertainty simulation functions defined ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_baseline",
   "metadata": {},
   "source": [
    "## 5) Train Baseline Models (multi-dataset)\n",
    "\n",
    "Trained on all 3 training datasets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_baseline_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model = VanillaNODE(hidden_dim=128)\n",
    "vanilla_model = train_model_multi(vanilla_model, \"Vanilla NODE\", epochs=5000, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_enhanced",
   "metadata": {},
   "source": [
    "## 6) Train Enhanced Models (multi-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_ctesn_enhanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "esn_kinematic = ContinuousTimeESN_Kinematic(\n",
    "    state_dim=2, input_dim=1, reservoir_dim=200,\n",
    "    spectral_radius=0.95, input_scale=0.5, leak_rate=1.0,\n",
    ")\n",
    "# Longer horizon (K=40) helps reservoir build up useful hidden state\n",
    "esn_kinematic = train_ctesn_multi(esn_kinematic, \"CT-ESN-Kinematic\",\n",
    "                                  epochs=5000, lr=0.005, k_steps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_cnode_enhanced",
   "metadata": {},
   "source": [
    "### 6b) Structured NODE & Adaptive NODE (multi-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_cnode_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_node = StructuredNODE(hidden_dim=128)\n",
    "structured_node = train_model_multi(structured_node, \"Structured NODE\", epochs=5000, lr=0.01)\n",
    "\n",
    "adaptive_node = AdaptiveNODE(hidden_dim=128)\n",
    "adaptive_node = train_model_multi(adaptive_node, \"Adaptive NODE\", epochs=5000, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural CDE training (commented out — not used) ---\n",
    "# cde_model = NeuralCDE(hidden_dim=64, input_channels=2)\n",
    "# cde_model = train_cde_multi(cde_model, \"Neural CDE\", epochs=10000, lr=0.001, k_steps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20347f",
   "metadata": {},
   "source": [
    "### 6c) Bayesian NODE (multi-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42038d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_node = BayesianNODE(hidden_dim=128, prior_sigma=1.0)\n",
    "bayesian_node = train_bnn_multi(\n",
    "    bayesian_node, \"Bayesian NODE\",\n",
    "    epochs=5000, lr=0.005, k_steps=20, batch_size=128,\n",
    "    kl_weight=1e-4, n_samples=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "## 7) Simulate & Evaluate — Training Data + Holdout Test Data\n",
    "\n",
    "Models are simulated on:\n",
    "- **Training data** (multisine_05) — to check fitting quality\n",
    "- **Holdout test data** (random_steps_01, random_steps_02) — to check generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulate_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Model registry ----------\n",
    "# type: \"ode\" (standard NODE), \"esn\" (CT-ESN)\n",
    "model_registry = {\n",
    "    \"Vanilla NODE\":     (vanilla_model,    \"ode\"),\n",
    "    \"CT-ESN-Kin.\":      (esn_kinematic,    \"esn\"),\n",
    "    \"Structured NODE\":  (structured_node,  \"ode\"),\n",
    "    \"Adaptive NODE\":    (adaptive_node,    \"ode\"),\n",
    "    \"Bayesian NODE\":    (bayesian_node,    \"ode\"),\n",
    "    # \"Neural CDE\":       (cde_model,        \"cde\"),  # commented out\n",
    "}\n",
    "\n",
    "# ---------- Evaluate on ALL datasets (train + test) ----------\n",
    "eval_datasets = (\n",
    "    [(\"train\", ds) for ds in train_tensors] +\n",
    "    [(\"test\",  ds) for ds in test_tensors]\n",
    ")\n",
    "\n",
    "results = {}   # results[model_name][split][dataset_name] = metrics dict\n",
    "\n",
    "for model_name, (model, mtype) in model_registry.items():\n",
    "    results[model_name] = {\"train\": {}, \"test\": {}}\n",
    "    for split, ds in eval_datasets:\n",
    "        t_np, y_true, y_pred = simulate_model(\n",
    "            model, ds, is_esn=(mtype == \"esn\"),\n",
    "            esn_model=esn_kinematic if mtype == \"esn\" else None,\n",
    "        )\n",
    "        m = compute_metrics(y_true, y_pred)\n",
    "        m[\"t\"] = t_np; m[\"y_true\"] = y_true; m[\"y_pred\"] = y_pred\n",
    "        results[model_name][split][ds[\"name\"]] = m\n",
    "\n",
    "print(\"Simulation & evaluation complete!\")\n",
    "\n",
    "# ---------- Summary Table ----------\n",
    "def print_metrics_table(results, split, metric_key=\"fit_pos\", fmt=\".1f\"):\n",
    "    ds_names = list(next(iter(results.values()))[split].keys())\n",
    "    header = f\"{'Model':<20}\" + \"\".join(f\"{n:>16}\" for n in ds_names) + f\"{'Mean':>10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for model_name in results:\n",
    "        vals = [results[model_name][split][dn][metric_key] for dn in ds_names]\n",
    "        row = f\"{model_name:<20}\" + \"\".join(f\"{v:>16{fmt}}\" for v in vals)\n",
    "        row += f\"{np.mean(vals):>10{fmt}}\"\n",
    "        print(row)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA — FIT% (Position)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"train\", \"fit_pos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA — R² (Position)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"train\", \"r2_pos\", fmt=\".4f\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA — R² (Velocity)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"train\", \"r2_vel\", fmt=\".4f\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOLDOUT TEST DATA — FIT% (Position)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"test\", \"fit_pos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOLDOUT TEST DATA — FIT% (Velocity)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"test\", \"fit_vel\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOLDOUT TEST DATA — R² (Position)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"test\", \"r2_pos\", fmt=\".4f\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOLDOUT TEST DATA — R² (Velocity)\")\n",
    "print(\"=\"*80)\n",
    "print_metrics_table(results, \"test\", \"r2_vel\", fmt=\".4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate",
   "metadata": {},
   "source": [
    "## 8) Visualisation — Train vs Holdout Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"Vanilla NODE\": \"tab:green\",\n",
    "    \"CT-ESN-Kin.\": \"tab:red\", \"Structured NODE\": \"tab:purple\",\n",
    "    \"Adaptive NODE\": \"tab:brown\",\n",
    "    \"Bayesian NODE\": \"tab:cyan\",\n",
    "    # \"Neural CDE\": \"tab:olive\",  # commented out\n",
    "}\n",
    "styles = {\n",
    "    \"Vanilla NODE\": \":\",\n",
    "    \"CT-ESN-Kin.\": \"--\", \"Structured NODE\": \"-.\",\n",
    "    \"Adaptive NODE\": (0, (3, 1, 1, 1)),\n",
    "    \"Bayesian NODE\": (0, (5, 2)),\n",
    "    # \"Neural CDE\": (0, (5, 1)),  # commented out\n",
    "}\n",
    "\n",
    "def plot_comparison(results, split, ds_name, title_suffix=\"\"):\n",
    "    \"\"\"Plot position + velocity comparison for one dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n",
    "    r0 = next(iter(results.values()))[split][ds_name]\n",
    "    t_plot = r0[\"t\"]\n",
    "    y_true = r0[\"y_true\"]\n",
    "\n",
    "    for ax_idx, (signal, ylabel) in enumerate([(0, \"Position\"), (1, \"Velocity\")]):\n",
    "        ax = axes[ax_idx]\n",
    "        ax.plot(t_plot, y_true[:, signal], 'k-', alpha=0.6, lw=2.5, label='Measured', zorder=10)\n",
    "        for model_name in results:\n",
    "            r = results[model_name][split][ds_name]\n",
    "            fit_key = \"fit_pos\" if signal == 0 else \"fit_vel\"\n",
    "            ax.plot(t_plot, r[\"y_pred\"][:, signal],\n",
    "                    linestyle=styles[model_name], color=colors[model_name], lw=1.5,\n",
    "                    label=f\"{model_name} ({r[fit_key]:.1f}%)\")\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        ax.legend(fontsize=9, loc=\"best\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    axes[0].set_title(f\"{ds_name} [{split.upper()}]{title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Time (s)\", fontsize=12)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- Plot one representative training dataset ---\n",
    "plot_comparison(results, \"train\", train_tensors[0][\"name\"])\n",
    "\n",
    "# --- Plot each holdout test dataset ---\n",
    "for ds in test_tensors:\n",
    "    plot_comparison(results, \"test\", ds[\"name\"], \" — HOLDOUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots",
   "metadata": {},
   "source": [
    "## 9) Train vs Test Summary Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grouped bar chart: Train vs Test FIT% and R² ---\n",
    "model_names = list(results.keys())\n",
    "train_ds_names = [ds[\"name\"] for ds in train_tensors]\n",
    "test_ds_names = [ds[\"name\"] for ds in test_tensors]\n",
    "\n",
    "train_fit = [np.mean([results[m][\"train\"][dn][\"fit_pos\"] for dn in train_ds_names]) for m in model_names]\n",
    "test_fit  = [np.mean([results[m][\"test\"][dn][\"fit_pos\"]  for dn in test_ds_names])  for m in model_names]\n",
    "train_r2  = [np.mean([results[m][\"train\"][dn][\"r2_pos\"]  for dn in train_ds_names]) for m in model_names]\n",
    "test_r2   = [np.mean([results[m][\"test\"][dn][\"r2_pos\"]   for dn in test_ds_names])  for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "w = 0.35\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# --- FIT% bar chart ---\n",
    "bars1 = ax1.bar(x - w/2, train_fit, w, label='Train (mean)', color='steelblue', alpha=0.85)\n",
    "bars2 = ax1.bar(x + w/2, test_fit,  w, label='Test (mean)',  color='tomato',    alpha=0.85)\n",
    "ax1.set_ylabel('FIT% (Position)', fontsize=12)\n",
    "ax1.set_title('FIT% — Train vs Holdout Test', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names, rotation=15, ha='right', fontsize=11)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for bar in bars1:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# --- R² bar chart ---\n",
    "bars3 = ax2.bar(x - w/2, train_r2, w, label='Train (mean)', color='steelblue', alpha=0.85)\n",
    "bars4 = ax2.bar(x + w/2, test_r2,  w, label='Test (mean)',  color='tomato',    alpha=0.85)\n",
    "ax2.set_ylabel('R² (Position)', fontsize=12)\n",
    "ax2.set_title('R² — Train vs Holdout Test', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right', fontsize=11)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(None, 1.05)\n",
    "for bar in bars3:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars4:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda6024",
   "metadata": {},
   "source": [
    "## 10) Bayesian NODE — Uncertainty Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c078d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bayesian NODE: Uncertainty visualisation on train + test ---\n",
    "N_MC = 30  # number of MC forward passes\n",
    "\n",
    "bnn_eval_datasets = (\n",
    "    [(\"train\", ds) for ds in train_tensors[:1]] +\n",
    "    [(\"test\",  ds) for ds in test_tensors]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(len(bnn_eval_datasets), 2, figsize=(16, 5 * len(bnn_eval_datasets)),\n",
    "                          sharex=False, squeeze=False)\n",
    "\n",
    "for i, (split, ds) in enumerate(bnn_eval_datasets):\n",
    "    t_np, y_true, mean_pred, std_pred = simulate_bnn_with_uncertainty(\n",
    "        bayesian_node, ds, n_mc=N_MC,\n",
    "    )\n",
    "\n",
    "    for j, (signal, ylabel) in enumerate([(0, \"Position\"), (1, \"Velocity\")]):\n",
    "        ax = axes[i, j]\n",
    "        ax.plot(t_np, y_true[:, signal], 'k-', lw=2, alpha=0.7, label='Measured')\n",
    "        ax.plot(t_np, mean_pred[:, signal], 'tab:cyan', lw=1.5, label='BNN mean')\n",
    "        ax.fill_between(\n",
    "            t_np,\n",
    "            mean_pred[:, signal] - 2 * std_pred[:, signal],\n",
    "            mean_pred[:, signal] + 2 * std_pred[:, signal],\n",
    "            color='tab:cyan', alpha=0.25, label='±2σ',\n",
    "        )\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        ax.legend(fontsize=9, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[i, 0].set_title(\n",
    "        f\"{ds['name']} [{split.upper()}] — Bayesian NODE uncertainty\",\n",
    "        fontsize=13, fontweight='bold',\n",
    "    )\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bayesian NODE uncertainty visualisation complete  (N_MC={N_MC})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c53acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as _time\n",
    "\n",
    "def ablation_train_and_eval(k_steps, batch_size, epochs=800, lr=0.01,\n",
    "                            full_sim=False):\n",
    "    \"\"\"\n",
    "    Train a fresh Structured-BB with given K/B, then evaluate on train + test.\n",
    "    If full_sim=True, use full-trajectory loss (single IC, entire dataset).\n",
    "    \"\"\"\n",
    "    model = StructuredNODE(hidden_dim=128).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    t0 = _time.time()\n",
    "\n",
    "    if full_sim:\n",
    "        # ------- Full-simulation loss (one dataset at a time) -------\n",
    "        for epoch in range(epochs + 1):\n",
    "            ds = train_tensors[np.random.randint(len(train_tensors))]\n",
    "            t_ds, u_ds, y_ds = ds[\"t\"], ds[\"u\"], ds[\"y\"]\n",
    "            model.u_series, model.t_series = u_ds, t_ds\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model.batch_start_times = torch.zeros(1, 1).to(device)\n",
    "            x0 = y_ds[0].unsqueeze(0)\n",
    "            pred = odeint(model, x0, t_ds, method='rk4')  # (T, 1, 2)\n",
    "            loss = torch.mean((pred.squeeze(1) - y_ds) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"  [full-sim] Epoch {epoch:>5d} | Loss: {loss.item():.6f}\")\n",
    "    else:\n",
    "        # ------- Multiple-shooting loss -------\n",
    "        for epoch in range(epochs + 1):\n",
    "            ds = train_tensors[np.random.randint(len(train_tensors))]\n",
    "            t_ds, u_ds, y_ds = ds[\"t\"], ds[\"u\"], ds[\"y\"]\n",
    "            dt_local = (t_ds[1] - t_ds[0]).item()\n",
    "            t_eval = torch.arange(0, k_steps * dt_local, dt_local, device=device)\n",
    "            model.u_series, model.t_series = u_ds, t_ds\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            start_idx = np.random.randint(0, len(t_ds) - k_steps, size=batch_size)\n",
    "            x0 = y_ds[start_idx]\n",
    "            model.batch_start_times = t_ds[start_idx].reshape(-1, 1)\n",
    "\n",
    "            pred_state = odeint(model, x0, t_eval, method='rk4')\n",
    "            batch_targets = torch.stack([y_ds[i:i + k_steps] for i in start_idx], dim=1)\n",
    "            loss = torch.mean((pred_state - batch_targets) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    wall_time = _time.time() - t0\n",
    "\n",
    "    # Evaluate on first train dataset + first test dataset\n",
    "    model.eval()\n",
    "    train_m = {}\n",
    "    for ds in train_tensors[:1]:\n",
    "        _, y_true, y_pred = simulate_model(model, ds)\n",
    "        train_m[ds[\"name\"]] = compute_metrics(y_true, y_pred)\n",
    "\n",
    "    test_m = {}\n",
    "    for ds in test_tensors[:1]:\n",
    "        _, y_true, y_pred = simulate_model(model, ds)\n",
    "        test_m[ds[\"name\"]] = compute_metrics(y_true, y_pred)\n",
    "\n",
    "    train_fit = np.mean([m[\"fit_pos\"] for m in train_m.values()])\n",
    "    test_fit  = np.mean([m[\"fit_pos\"] for m in test_m.values()])\n",
    "\n",
    "    return train_fit, test_fit, wall_time\n",
    "\n",
    "# ---------- Run ablation ----------\n",
    "K_values = [10, 20, 50, 100]\n",
    "B_values = [64, 128, 256]\n",
    "ABLATION_EPOCHS = 800\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ABLATION: K-steps × Batch-size  (Structured-BB, {} epochs)\".format(ABLATION_EPOCHS))\n",
    "print(\"=\"*80)\n",
    "\n",
    "for K in K_values:\n",
    "    for B in B_values:\n",
    "        label = f\"K={K:>3d}, B={B:>3d}\"\n",
    "        print(f\"\\n--- {label} ---\")\n",
    "        tr, te, wt = ablation_train_and_eval(K, B, epochs=ABLATION_EPOCHS)\n",
    "        ablation_results.append({\"K\": K, \"B\": B, \"train_fit\": tr, \"test_fit\": te,\n",
    "                                  \"wall_s\": wt, \"label\": label})\n",
    "        print(f\"  → Train FIT={tr:.1f}%  |  Test FIT={te:.1f}%  |  Time={wt:.1f}s\")\n",
    "\n",
    "# ---------- Full-simulation loss ----------\n",
    "print(f\"\\n--- Full-simulation loss (K=all) ---\")\n",
    "tr, te, wt = ablation_train_and_eval(None, None, epochs=min(ABLATION_EPOCHS, 400),\n",
    "                                      full_sim=True)\n",
    "ablation_results.append({\"K\": \"full\", \"B\": 1, \"train_fit\": tr, \"test_fit\": te,\n",
    "                          \"wall_s\": wt, \"label\": \"K=full, B=1\"})\n",
    "print(f\"  → Train FIT={tr:.1f}%  |  Test FIT={te:.1f}%  |  Time={wt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5409ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ablation Results: Table + Heatmap ──\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_abl = pd.DataFrame(ablation_results)\n",
    "\n",
    "# ── Pretty table ──\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY RESULTS — Structured NODE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Config':<20s} {'Train FIT%':>12s} {'Test FIT%':>12s} {'Time (s)':>10s}\")\n",
    "print(\"-\"*56)\n",
    "for _, r in df_abl.iterrows():\n",
    "    print(f\"{r['label']:<20s} {r['train_fit']:>11.1f}% {r['test_fit']:>11.1f}% {r['wall_s']:>9.1f}\")\n",
    "print(\"-\"*56)\n",
    "\n",
    "# ── Heatmaps: Train FIT% and Test FIT% ──\n",
    "df_grid = df_abl[df_abl[\"K\"] != \"full\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, metric, title in zip(axes[:2],\n",
    "                              [\"train_fit\", \"test_fit\"],\n",
    "                              [\"Train FIT %\", \"Test FIT %  (holdout)\"]):\n",
    "    pivot = df_grid.pivot(index=\"K\", columns=\"B\", values=metric)\n",
    "    pivot = pivot.sort_index(ascending=True)\n",
    "    im = ax.imshow(pivot.values, cmap=\"RdYlGn\", aspect=\"auto\",\n",
    "                   vmin=0, vmax=100)\n",
    "    ax.set_xticks(range(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns)\n",
    "    ax.set_yticks(range(len(pivot.index)))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    ax.set_xlabel(\"Batch size B\")\n",
    "    ax.set_ylabel(\"Prediction horizon K\")\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    for i in range(len(pivot.index)):\n",
    "        for j in range(len(pivot.columns)):\n",
    "            val = pivot.values[i, j]\n",
    "            ax.text(j, i, f\"{val:.1f}\", ha=\"center\", va=\"center\",\n",
    "                    color=\"black\" if 30 < val < 80 else \"white\", fontsize=11)\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# ── Wall-clock time ──\n",
    "ax3 = axes[2]\n",
    "pivot_t = df_grid.pivot(index=\"K\", columns=\"B\", values=\"wall_s\").sort_index()\n",
    "im3 = ax3.imshow(pivot_t.values, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "ax3.set_xticks(range(len(pivot_t.columns)))\n",
    "ax3.set_xticklabels(pivot_t.columns)\n",
    "ax3.set_yticks(range(len(pivot_t.index)))\n",
    "ax3.set_yticklabels(pivot_t.index)\n",
    "ax3.set_xlabel(\"Batch size B\")\n",
    "ax3.set_ylabel(\"Prediction horizon K\")\n",
    "ax3.set_title(\"Training wall-clock time (s)\", fontsize=13)\n",
    "for i in range(len(pivot_t.index)):\n",
    "    for j in range(len(pivot_t.columns)):\n",
    "        v = pivot_t.values[i, j]\n",
    "        ax3.text(j, i, f\"{v:.0f}\", ha=\"center\", va=\"center\", fontsize=11)\n",
    "fig.colorbar(im3, ax=ax3, shrink=0.8)\n",
    "\n",
    "plt.suptitle(\"Ablation Study: K-step Prediction Horizon × Batch Size\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ablation_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ── Highlight full-simulation row ──\n",
    "full_row = df_abl[df_abl[\"K\"] == \"full\"]\n",
    "if not full_row.empty:\n",
    "    r = full_row.iloc[0]\n",
    "    print(f\"\\n★ Full-simulation baseline:  Train FIT = {r['train_fit']:.1f}%\"\n",
    "          f\"  |  Test FIT = {r['test_fit']:.1f}%  |  Time = {r['wall_s']:.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
